{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ASR Demo Notebook\n",
        "\n",
        "This notebook demonstrates how to use the ASR (Automatic Speech Recognition) model.\n",
        "\n",
        "**Features:**\n",
        "- Clone repository and install dependencies\n",
        "- Download pre-trained model weights\n",
        "- Run inference on custom data\n",
        "- Calculate WER/CER metrics\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your actual repo URL)\n",
        "!git clone https://github.com/YOUR_USERNAME/asr-project.git\n",
        "%cd asr-project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: https://drive.google.com/uc?id=1j9e24ERS2cPsPC1zqabkJjH3E-72LcuW\n",
            "Model downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Create directory for saved models\n",
        "os.makedirs(\"saved\", exist_ok=True)\n",
        "\n",
        "# Download model weights from Google Drive\n",
        "# Replace YOUR_MODEL_ID with actual Google Drive file ID\n",
        "MODEL_GDRIVE_ID = \"1j9e24ERS2cPsPC1zqabkJjH3E-72LcuW\"  # <-- REPLACE THIS\n",
        "\n",
        "!gdown https://drive.google.com/uc?id={MODEL_GDRIVE_ID} -O saved/model_best.pth\n",
        "print(\"Model downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run Inference on Custom Data\n",
        "\n",
        "### 2.1 Mount Google Drive and set data path\n",
        "\n",
        "Prepare your data in the following format:\n",
        "```\n",
        "your_data/\n",
        "├── audio/\n",
        "│   ├── file1.wav\n",
        "    ├── file2.wav\n",
        "│   └── ...\n",
        "└── transcriptions/  (optional)\n",
        "    ├── file1.txt\n",
        "    ├── file2.txt\n",
        "    └── ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Mount Google Drive\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      3\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data path: data/custom_data/\n",
            "Audio files found: ['audio_4.wav', 'audio_5.wav', 'audio_7.wav', 'audio_6.wav', 'audio_2.wav']...\n"
          ]
        }
      ],
      "source": [
        "# Set path to your custom data\n",
        "# Example: /content/drive/MyDrive/HSE_ASR_project/custom_data\n",
        "# CUSTOM_DATA_PATH = \"/content/drive/MyDrive/YOUR_DATA_FOLDER\"  # <-- CHANGE THIS\n",
        "CUSTOM_DATA_PATH = 'data/custom_data/'\n",
        "\n",
        "print(f\"Data path: {CUSTOM_DATA_PATH}\")\n",
        "if os.path.exists(os.path.join(CUSTOM_DATA_PATH, 'audio')):\n",
        "    audio_files = os.listdir(os.path.join(CUSTOM_DATA_PATH, 'audio'))[:5]\n",
        "    print(f\"Audio files found: {audio_files}...\")\n",
        "else:\n",
        "    print(\"ERROR: audio/ folder not found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DeepSpeech2(\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): MaskConv2d(\n",
            "      (conv): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
            "    )\n",
            "    (1): MaskConv2d(\n",
            "      (conv): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
            "    )\n",
            "  )\n",
            "  (conv_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_activation): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
            "  (rnn_layers): ModuleList(\n",
            "    (0): BatchRNN(\n",
            "      (rnn): GRU(640, 256, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "    (1-2): 2 x BatchRNN(\n",
            "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (rnn): GRU(512, 256, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=29, bias=True)\n",
            ")\n",
            "All parameters: 4,012,797\n",
            "Trainable parameters: 4,012,797\n",
            "Loading model weights from: saved/model_best.pth ...\n",
            "inference: 100%|██████████████████████████████████| 1/1 [00:01<00:00,  1.28s/it]\n",
            "    inference_WER  : 1.6666666666666667\n",
            "    inference_CER  : 1.3838095238095238\n"
          ]
        }
      ],
      "source": [
        "# Run inference on custom data\n",
        "!python inference.py -cn=asr_inference_custom \\\n",
        "    \"datasets.inference.data_dir={CUSTOM_DATA_PATH}\" \\\n",
        "    inferencer.from_pretrained=saved/model_best.pth \\\n",
        "    inferencer.save_path=custom_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. View Predictions and Calculate Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample predictions:\n",
            "\n",
            "[audio_7]\n",
            "  nraseoer gol\n",
            "\n",
            "[audio_6]\n",
            "  norier qapcer qipool\n",
            "\n",
            "[audio_4]\n",
            "  krir'ar a ar aalrk\n",
            "\n",
            "[audio_5]\n",
            "  k f ar ahogw ol\n",
            "\n",
            "[audio_1]\n",
            "  nral\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Show predictions\n",
        "predictions_dir = Path(\"data/saved/custom_results/inference\")\n",
        "\n",
        "if predictions_dir.exists():\n",
        "    pred_files = list(predictions_dir.glob(\"*.txt\"))[:5]\n",
        "    \n",
        "    print(\"Sample predictions:\\n\")\n",
        "    for pred_file in pred_files:\n",
        "        with open(pred_file, 'r') as f:\n",
        "            prediction = f.read()\n",
        "        print(f\"[{pred_file.stem}]\")\n",
        "        print(f\"  {prediction}\\n\")\n",
        "else:\n",
        "    raise Exception(\"No predictions found. Please run inference first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading predictions from: data/saved/custom_results/inference\n",
            "Loading ground truth from: data/custom_data/transcriptions\n",
            "\n",
            "Found 10 predictions\n",
            "Found 10 ground truth files\n",
            "\n",
            "[audio_0]\n",
            "  GT:   жизнь других\n",
            "  Pred: ga fcer gopck\n",
            "  WER: 150.00%, CER: 108.33%\n",
            "\n",
            "[audio_1]\n",
            "  GT:   элтон джонс\n",
            "  Pred: nral\n",
            "  WER: 100.00%, CER: 100.00%\n",
            "\n",
            "[audio_2]\n",
            "  GT:   побег из шоушенко\n",
            "  Pred: nherhaer f f ak gol\n",
            "  WER: 166.67%, CER: 105.88%\n",
            "\n",
            "[audio_3]\n",
            "  GT:   дайан вормик\n",
            "  Pred: nhraokqporan h ael\n",
            "  WER: 150.00%, CER: 150.00%\n",
            "\n",
            "[audio_4]\n",
            "  GT:   мухаммед али\n",
            "  Pred: krir'ar a ar aalrk\n",
            "  WER: 200.00%, CER: 141.67%\n",
            "\n",
            "[audio_5]\n",
            "  GT:   дневник памяти\n",
            "  Pred: k f ar ahogw ol\n",
            "  WER: 250.00%, CER: 107.14%\n",
            "\n",
            "[audio_6]\n",
            "  GT:   рэйф файнс\n",
            "  Pred: norier qapcer qipool\n",
            "  WER: 150.00%, CER: 190.00%\n",
            "\n",
            "[audio_7]\n",
            "  GT:   нефть\n",
            "  Pred: nraseoer gol\n",
            "  WER: 200.00%, CER: 240.00%\n",
            "\n",
            "[audio_8]\n",
            "  GT:   золотая лихорадка\n",
            "  Pred: nier ar acanh aer\n",
            "  WER: 200.00%, CER: 94.12%\n",
            "\n",
            "[audio_9]\n",
            "  GT:   служебный роман\n",
            "  Pred: moil'pololqller qxlpool\n",
            "  WER: 100.00%, CER: 146.67%\n",
            "\n",
            "==================================================\n",
            "RESULTS\n",
            "==================================================\n",
            "Matched utterances: 10\n",
            "Missing predictions: 0\n",
            "Missing ground truth: 0\n",
            "\n",
            "Average WER: 166.67%\n",
            "Average CER: 138.38%\n",
            "\n",
            "Corpus WER: 165.00% (33/20 words)\n",
            "Corpus CER: 128.80% (161/125 chars)\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Calculate metrics (if ground truth transcriptions are available)\n",
        "GROUND_TRUTH_PATH = f\"{CUSTOM_DATA_PATH}/transcriptions\"\n",
        "PREDICTIONS_PATH = \"data/saved/custom_results/inference\"\n",
        "\n",
        "!python calc_metrics.py \\\n",
        "    --predictions \"{PREDICTIONS_PATH}\" \\\n",
        "    --ground_truth \"{GROUND_TRUTH_PATH}\" \\\n",
        "    --verbose\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Interactive Demo - Transcribe a Single Audio File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[32m     20\u001b[39m model = DeepSpeech2(\n\u001b[32m     21\u001b[39m     n_feats=\u001b[32m80\u001b[39m,\n\u001b[32m     22\u001b[39m     n_tokens=\u001b[38;5;28mlen\u001b[39m(text_encoder),\n\u001b[32m     23\u001b[39m ).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaved/model_best.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m model.load_state_dict(checkpoint[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     27\u001b[39m model.eval()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git_projects/automatic-speech-recognition-project/venv/lib/python3.13/site-packages/torch/serialization.py:1529\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1521\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1522\u001b[39m                     opened_zipfile,\n\u001b[32m   1523\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1526\u001b[39m                     **pickle_load_args,\n\u001b[32m   1527\u001b[39m                 )\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1531\u001b[39m             opened_zipfile,\n\u001b[32m   1532\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1535\u001b[39m             **pickle_load_args,\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
            "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from IPython.display import Audio, display\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from src.model import DeepSpeech2\n",
        "from src.text_encoder import TextEncoder\n",
        "from src.transforms import MelSpectrogram\n",
        "\n",
        "# Load model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize components\n",
        "text_encoder = TextEncoder()\n",
        "mel_spec = MelSpectrogram()\n",
        "\n",
        "# Load model\n",
        "model = DeepSpeech2(\n",
        "    n_feats=80,\n",
        "    n_tokens=len(text_encoder),\n",
        ").to(device)\n",
        "\n",
        "checkpoint = torch.load(\"saved/model_best.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe(audio_path: str, use_beam_search: bool = False, beam_size: int = 10):\n",
        "    \"\"\"Transcribe an audio file.\"\"\"\n",
        "    # Load audio\n",
        "    audio, sr = torchaudio.load(audio_path)\n",
        "    audio = audio.squeeze(0)\n",
        "    \n",
        "    # Resample if needed\n",
        "    if sr != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sr, 16000)\n",
        "        audio = resampler(audio)\n",
        "    \n",
        "    # Compute spectrogram\n",
        "    spectrogram = mel_spec(audio)\n",
        "    spectrogram = spectrogram.unsqueeze(0).to(device)\n",
        "    spec_length = torch.tensor([spectrogram.shape[-1]]).to(device)\n",
        "    \n",
        "    # Run model\n",
        "    with torch.no_grad():\n",
        "        output = model(spectrogram=spectrogram, spectrogram_length=spec_length)\n",
        "    \n",
        "    log_probs = output[\"log_probs\"]\n",
        "    log_probs_length = output[\"log_probs_length\"]\n",
        "    \n",
        "    # Decode\n",
        "    if use_beam_search:\n",
        "        return text_encoder.ctc_beam_search(log_probs, log_probs_length, beam_size=beam_size)[0]\n",
        "    return text_encoder.ctc_decode(log_probs, log_probs_length)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload an audio file and transcribe\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload an audio file (wav, flac, or mp3):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"\\nProcessing: {filename}\")\n",
        "    display(Audio(filename))\n",
        "    \n",
        "    print(\"\\nTranscription (Greedy):\")\n",
        "    print(f\"  {transcribe(filename, use_beam_search=False)}\")\n",
        "    \n",
        "    print(\"\\nTranscription (Beam Search):\")\n",
        "    print(f\"  {transcribe(filename, use_beam_search=True, beam_size=10)}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
