{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6ApjE1N8TR_",
        "outputId": "d431b3aa-356f-48ed-97b3-31c6db1318ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'test'...\n",
            "remote: Enumerating objects: 160, done.\u001b[K\n",
            "remote: Counting objects: 100% (160/160), done.\u001b[K\n",
            "remote: Compressing objects: 100% (148/148), done.\u001b[K\n",
            "remote: Total 160 (delta 9), reused 157 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (160/160), 160.12 KiB | 20.01 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Lak1n26/test.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWzJ_RjI8YFI",
        "outputId": "67258ab9-bff7-4cf0-988b-1745fa365e47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb6dPYCq8XTs",
        "outputId": "b9a8947e-5d3c-4a03-d8a9-4c0ee4e25c0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calc_metrics.py  inference.py  requirements.txt  train.py\n",
            "demo.ipynb       LICENSE       \u001b[0m\u001b[01;34msrc\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "GeWjxI3x8dZG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pre-commit install"
      ],
      "metadata": {
        "id": "32e9vtWk81zd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0rjYg2G8dXA",
        "outputId": "448cde34-fdc8-4e62-e8a5-4d95eb7e81cf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py -cn=asr_baseline trainer.override=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1C0KQJO8dU3",
        "outputId": "78035675-6b43-4cfe-80db-21c015d7b045"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding save directory '/content/test/saved/asr_training'...\n",
            "Logging git commit and patch...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlakin26\u001b[0m (\u001b[33mlakin26-projects\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run lz423k7g (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run lz423k7g (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run lz423k7g (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m setting up run lz423k7g (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m setting up run lz423k7g (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m setting up run lz423k7g (0.6s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/test/wandb/run-20251225_104649-lz423k7g\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33masr_training\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lakin26-projects/asr_project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lakin26-projects/asr_project/runs/lz423k7g\u001b[0m\n",
            "Dataset part 'train-clean-100' already exists at /content/test/data/datasets/librispeech/LibriSpeech/train-clean-100\n",
            "Filtered index from 28539 to 28538 items (max_audio=20.0s, max_text=None)\n",
            "Dataset part 'dev-clean' already exists at /content/test/data/datasets/librispeech/LibriSpeech/dev-clean\n",
            "DeepSpeech2(\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): MaskConv2d(\n",
            "      (conv): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
            "    )\n",
            "    (1): MaskConv2d(\n",
            "      (conv): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
            "    )\n",
            "  )\n",
            "  (conv_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_activation): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
            "  (rnn_layers): ModuleList(\n",
            "    (0): BatchRNN(\n",
            "      (rnn): GRU(640, 512, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "    (1-4): 4 x BatchRNN(\n",
            "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (rnn): GRU(1024, 512, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=29, bias=True)\n",
            ")\n",
            "All parameters: 22,733,053\n",
            "Trainable parameters: 22,733,053\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 1 [0/100 (0%)] Loss: 8.144608\n",
            "[DEBUG] Prediction sample: 'yyhyjejeqeqrjrjemevtevtvgttldjzxcgclulbkn b b zcgc...' | Target: 'then marian barber dropped a neat field goal and s...'\n",
            "train:  29% 29/100 [01:32<03:46,  3.19s/it]\n",
            "Saving model on keyboard interrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "exQWaxrDCW20"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py -cn=asr_onebatch trainer.override=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvQ2pHLA8dP7",
        "outputId": "1177f487-68d7-4299-b2fe-f909bb31462f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding save directory '/content/test/saved/asr_training'...\n",
            "Logging git commit and patch...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlakin26\u001b[0m (\u001b[33mlakin26-projects\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run 7ml2j0mv (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run 7ml2j0mv (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run 7ml2j0mv (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m setting up run 7ml2j0mv (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m setting up run 7ml2j0mv (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m setting up run 7ml2j0mv (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/test/wandb/run-20251225_111003-7ml2j0mv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33masr_training\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lakin26-projects/asr_project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lakin26-projects/asr_project/runs/7ml2j0mv\u001b[0m\n",
            "Dataset part 'dev-clean' already exists at /content/test/data/datasets/librispeech/LibriSpeech/dev-clean\n",
            "Filtered index from 2703 to 2136 items (max_audio=10.0s, max_text=None)\n",
            "Dataset part 'dev-clean' already exists at /content/test/data/datasets/librispeech/LibriSpeech/dev-clean\n",
            "Filtered index from 2703 to 2136 items (max_audio=10.0s, max_text=None)\n",
            "DeepSpeech2(\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): MaskConv2d(\n",
            "      (conv): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
            "    )\n",
            "    (1): MaskConv2d(\n",
            "      (conv): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
            "    )\n",
            "  )\n",
            "  (conv_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_activation): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
            "  (rnn_layers): ModuleList(\n",
            "    (0): BatchRNN(\n",
            "      (rnn): GRU(640, 256, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "    (1-2): 2 x BatchRNN(\n",
            "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (rnn): GRU(512, 256, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=29, bias=True)\n",
            ")\n",
            "All parameters: 4,012,797\n",
            "Trainable parameters: 4,012,797\n",
            "train:   0% 0/50 [00:00<?, ?it/s]Train Epoch: 1 [0/50 (0%)] Loss: 9.387247\n",
            "[DEBUG] Prediction sample: 'd' zuzpuopgudgdeka snszznjmj pxslnslrlscqsjkel'idi...' | Target: 'it is obviously unnecessary for us to point out ho...'\n",
            "train:  20% 10/50 [00:04<00:13,  3.02it/s]Train Epoch: 1 [10/50 (20%)] Loss: 2.974313\n",
            "[DEBUG] Prediction sample: 'ns is s s sists s str...' | Target: 'nor is mister quilter's manner less interesting th...'\n",
            "train:  40% 20/50 [00:07<00:09,  3.25it/s]Train Epoch: 1 [20/50 (40%)] Loss: 2.410538\n",
            "[DEBUG] Prediction sample: 'nio s tite eis els s to s e...' | Target: 'mister quilter is the apostle of the middle classe...'\n",
            "train:  60% 30/50 [00:10<00:05,  3.68it/s]Train Epoch: 1 [30/50 (60%)] Loss: 1.927889\n",
            "[DEBUG] Prediction sample: 'i a he re ic oeo ei l  id ri ats h ter...' | Target: 'he has grave doubts whether sir frederick leighton...'\n",
            "train:  80% 40/50 [00:13<00:02,  3.67it/s]Train Epoch: 1 [40/50 (80%)] Loss: 1.435337\n",
            "[DEBUG] Prediction sample: 'i sbisy unesrf t int w uinou he ics r lice inerese...' | Target: 'it is obviously unnecessary for us to point out ho...'\n",
            "train:  98% 49/50 [00:15<00:00,  3.08it/s]\n",
            "val: 100% 1/1 [00:00<00:00,  6.59it/s]\n",
            "[DEBUG] Prediction sample: 'iser qie i raiteftheidce cases ander ga toelecme h...' | Target: 'mister quilter is the apostle of the middle classe...'\n",
            "    epoch          : 1\n",
            "    loss           : 1.6593303561210633\n",
            "    grad_norm      : 0.7781123101711274\n",
            "    CER            : 0.5663707685255586\n",
            "    val_loss       : 1.6630589962005615\n",
            "    val_WER        : 0.9248774509803922\n",
            "    val_CER        : 0.42276786692073504\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/50 [00:00<?, ?it/s]Train Epoch: 2 [0/50 (0%)] Loss: 0.999281\n",
            "[DEBUG] Prediction sample: 'iser quilte i aoste h midle lases n e ae gad ecome...' | Target: 'mister quilter is the apostle of the middle classe...'\n",
            "train:  20% 10/50 [00:03<00:16,  2.49it/s]Train Epoch: 2 [10/50 (20%)] Loss: 0.676628\n",
            "[DEBUG] Prediction sample: 'iobvously unesaryfor us o oint tout how luminous t...' | Target: 'it is obviously unnecessary for us to point out ho...'\n",
            "train:  40% 20/50 [00:06<00:08,  3.58it/s]Train Epoch: 2 [20/50 (40%)] Loss: 0.461479\n",
            "[DEBUG] Prediction sample: 'nor i mister quilter's maner less intesting han hi...' | Target: 'nor is mister quilter's manner less interesting th...'\n",
            "train:  60% 30/50 [00:09<00:05,  3.55it/s]Train Epoch: 2 [30/50 (60%)] Loss: 0.308794\n",
            "[DEBUG] Prediction sample: 'mister quilter is the apostle of the middle classe...' | Target: 'mister quilter is the apostle of the middle classe...'\n",
            "train:  80% 40/50 [00:12<00:02,  3.54it/s]Train Epoch: 2 [40/50 (80%)] Loss: 0.210800\n",
            "[DEBUG] Prediction sample: 'nor is mister quilter's maner less interesting tha...' | Target: 'nor is mister quilter's manner less interesting th...'\n",
            "train:  98% 49/50 [00:15<00:00,  3.07it/s]\n",
            "val: 100% 1/1 [00:00<00:00,  4.71it/s]\n",
            "[DEBUG] Prediction sample: 'mister quilter is te apostle of the midle classes ...' | Target: 'mister quilter is the apostle of the middle classe...'\n",
            "    epoch          : 2\n",
            "    loss           : 0.2543087527155876\n",
            "    grad_norm      : 0.6244892418384552\n",
            "    CER            : 0.05675561406637693\n",
            "    val_loss       : 0.28614935278892517\n",
            "    val_WER        : 0.3301062091503268\n",
            "    val_CER        : 0.07899586669545273\n",
            "Saving current best: model_best.pth ...\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33masr_training\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251225_111003-7ml2j0mv/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 visualize_augmentations.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWjlbBpy8dNY",
        "outputId": "eea09f0d-2080-4803-c8f4-243e0e38b588"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using audio: data/datasets/librispeech/LibriSpeech/train-clean-100/4830/25904/4830-25904-0025.flac\n",
            "Audio shape: torch.Size([226640]), Sample rate: 16000\n",
            "Spectrogram shape: torch.Size([80, 1417])\n",
            "\n",
            "=== Audio Augmentations ===\n",
            "1. Gaussian Noise\n",
            "2. Gain (Volume Change)\n",
            "3. Speed Perturbation\n",
            "4. Pitch Shift\n",
            "\n",
            "=== Spectrogram Augmentations ===\n",
            "5. Frequency Masking\n",
            "6. Time Masking\n",
            "7. SpecAugment (Freq + Time Masking)\n",
            "8. Time Warp\n",
            "\n",
            "‚úì All visualizations saved to: augmentation_examples/\n",
            "\n",
            "Augmentations implemented:\n",
            "  Audio: GaussianNoise, Gain, SpeedPerturbation, PitchShift\n",
            "  Spectrogram: FrequencyMasking, TimeMasking, SpecAugment, TimeWarp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py -cn=asr_baseline trainer.override=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duRvN-3_8dJU",
        "outputId": "fe8ee635-de85-4750-f895-5d2fa531bcaa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding save directory '/content/test/saved/asr_training'...\n",
            "Logging git commit and patch...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlakin26\u001b[0m (\u001b[33mlakin26-projects\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m setting up run whpqkrtj (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m setting up run whpqkrtj (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m setting up run whpqkrtj (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m setting up run whpqkrtj (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m setting up run whpqkrtj (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m setting up run whpqkrtj (0.6s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m setting up run whpqkrtj (0.6s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/test/wandb/run-20251225_111611-whpqkrtj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33masr_training\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lakin26-projects/asr_project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lakin26-projects/asr_project/runs/whpqkrtj\u001b[0m\n",
            "Dataset part 'train-clean-100' already exists at /content/test/data/datasets/librispeech/LibriSpeech/train-clean-100\n",
            "Filtered index from 28539 to 28538 items (max_audio=20.0s, max_text=None)\n",
            "Dataset part 'dev-clean' already exists at /content/test/data/datasets/librispeech/LibriSpeech/dev-clean\n",
            "DeepSpeech2(\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): MaskConv2d(\n",
            "      (conv): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
            "    )\n",
            "    (1): MaskConv2d(\n",
            "      (conv): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
            "    )\n",
            "  )\n",
            "  (conv_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_activation): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
            "  (rnn_layers): ModuleList(\n",
            "    (0): BatchRNN(\n",
            "      (rnn): GRU(640, 512, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "    (1-4): 4 x BatchRNN(\n",
            "      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (rnn): GRU(1024, 512, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=29, bias=True)\n",
            ")\n",
            "All parameters: 22,733,053\n",
            "Trainable parameters: 22,733,053\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 1 [0/100 (0%)] Loss: 8.144608\n",
            "train:  99% 99/100 [04:42<00:02,  2.86s/it]\n",
            "val: 100% 85/85 [00:43<00:00,  1.97it/s]\n",
            "    epoch          : 1\n",
            "    loss           : 8.144608497619629\n",
            "    grad_norm      : 6.5928730964660645\n",
            "    CER            : 1.5017143280433514\n",
            "    val_loss       : 3.06623792367823\n",
            "    val_WER        : 1.1002271229594554\n",
            "    val_CER        : 0.7185305223890164\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 2 [0/100 (0%)] Loss: 2.738336\n",
            "train:  99% 99/100 [04:44<00:02,  2.87s/it]\n",
            "val: 100% 85/85 [00:45<00:00,  1.86it/s]\n",
            "    epoch          : 2\n",
            "    loss           : 2.7383360862731934\n",
            "    grad_norm      : 0.5732803344726562\n",
            "    CER            : 0.7475018153838975\n",
            "    val_loss       : 2.0169335870181815\n",
            "    val_WER        : 1.0374474820339432\n",
            "    val_CER        : 0.5877728077587836\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 3 [0/100 (0%)] Loss: 1.988013\n",
            "train:  99% 99/100 [04:54<00:02,  2.97s/it]\n",
            "val: 100% 85/85 [00:48<00:00,  1.76it/s]\n",
            "    epoch          : 3\n",
            "    loss           : 1.9880127906799316\n",
            "    grad_norm      : 1.5054084062576294\n",
            "    CER            : 0.5994109840394279\n",
            "    val_loss       : 1.553837251663208\n",
            "    val_WER        : 0.9835027181886935\n",
            "    val_CER        : 0.46570238162798927\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 4 [0/100 (0%)] Loss: 1.433165\n",
            "train:  99% 99/100 [04:58<00:03,  3.01s/it]\n",
            "val: 100% 85/85 [00:48<00:00,  1.77it/s]\n",
            "    epoch          : 4\n",
            "    loss           : 1.4331650733947754\n",
            "    grad_norm      : 1.4365205764770508\n",
            "    CER            : 0.4451668850832071\n",
            "    val_loss       : 1.3605795271256391\n",
            "    val_WER        : 0.916259397410513\n",
            "    val_CER        : 0.41407627949695736\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 5 [0/100 (0%)] Loss: 1.299416\n",
            "train:  99% 99/100 [04:58<00:03,  3.02s/it]\n",
            "val: 100% 85/85 [00:47<00:00,  1.78it/s]\n",
            "    epoch          : 5\n",
            "    loss           : 1.2994158267974854\n",
            "    grad_norm      : 1.587225317955017\n",
            "    CER            : 0.4011770585492181\n",
            "    val_loss       : 1.2119339473107282\n",
            "    val_WER        : 0.8642623077524358\n",
            "    val_CER        : 0.3784123910183555\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 6 [0/100 (0%)] Loss: 1.153165\n",
            "train:  99% 99/100 [04:55<00:02,  2.98s/it]\n",
            "val: 100% 85/85 [00:48<00:00,  1.75it/s]\n",
            "    epoch          : 6\n",
            "    loss           : 1.1531646251678467\n",
            "    grad_norm      : 1.2037333250045776\n",
            "    CER            : 0.361955530119769\n",
            "    val_loss       : 1.1240752556744744\n",
            "    val_WER        : 0.837693701270237\n",
            "    val_CER        : 0.34887240348398163\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 7 [0/100 (0%)] Loss: 1.149414\n",
            "train:  99% 99/100 [04:57<00:03,  3.01s/it]\n",
            "val: 100% 85/85 [00:48<00:00,  1.76it/s]\n",
            "    epoch          : 7\n",
            "    loss           : 1.149414300918579\n",
            "    grad_norm      : 1.3282816410064697\n",
            "    CER            : 0.3564320020094214\n",
            "    val_loss       : 1.0620999280144185\n",
            "    val_WER        : 0.8126197208379358\n",
            "    val_CER        : 0.3288303695487255\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 8 [0/100 (0%)] Loss: 1.016109\n",
            "train:  99% 99/100 [04:59<00:03,  3.02s/it]\n",
            "val: 100% 85/85 [00:48<00:00,  1.77it/s]\n",
            "    epoch          : 8\n",
            "    loss           : 1.0161089897155762\n",
            "    grad_norm      : 1.0928833484649658\n",
            "    CER            : 0.31974285636893496\n",
            "    val_loss       : 1.0257199476746952\n",
            "    val_WER        : 0.797275569975534\n",
            "    val_CER        : 0.3170946203092459\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 9 [0/100 (0%)] Loss: 0.955982\n",
            "train:  99% 99/100 [05:00<00:03,  3.04s/it]\n",
            "val: 100% 85/85 [00:48<00:00,  1.74it/s]\n",
            "    epoch          : 9\n",
            "    loss           : 0.9559816122055054\n",
            "    grad_norm      : 1.2192931175231934\n",
            "    CER            : 0.30083652532717436\n",
            "    val_loss       : 1.0117637416895697\n",
            "    val_WER        : 0.7914022794427402\n",
            "    val_CER        : 0.3142734042308618\n",
            "Saving current best: model_best.pth ...\n",
            "train:   0% 0/100 [00:00<?, ?it/s]Train Epoch: 10 [0/100 (0%)] Loss: 0.976504\n",
            "train:  99% 99/100 [04:57<00:03,  3.00s/it]\n",
            "val: 100% 85/85 [00:47<00:00,  1.78it/s]\n",
            "    epoch          : 10\n",
            "    loss           : 0.9765037298202515\n",
            "    grad_norm      : 1.2457362413406372\n",
            "    CER            : 0.3092484349857608\n",
            "    val_loss       : 1.0070629056762246\n",
            "    val_WER        : 0.7900003106057477\n",
            "    val_CER        : 0.3136332187241808\n",
            "Saving current best: model_best.pth ...\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33masr_training\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251225_111611-whpqkrtj/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2HYNOl-v8dDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1OB9FZzRRTbw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}